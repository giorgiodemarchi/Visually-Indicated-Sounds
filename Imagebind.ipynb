{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HheQoeWsfV4Y"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/giorgiodemarchi/ImageBind.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx2-xzC9hQ8R"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/giorgiodemarchi/Visually-Indicated-Sounds.git  ## This was done in a external colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4fW-tCh_wv",
        "outputId": "658f3039-3cc7-4c56-81fd-b008da79b7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.81-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.81 (from boto3)\n",
            "  Downloading botocore-1.34.81-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.81->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.81->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.81->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.81 botocore-1.34.81 jmespath-1.0.1 s3transfer-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAMLJmOvfrVI"
      },
      "outputs": [],
      "source": [
        "!cd ImageBind; pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHI9R5KVfK94",
        "outputId": "ac6cd899-294e-4b73-af1c-1c84162b3998"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from imagebind import data\n",
        "import torch\n",
        "\n",
        "from imagebind.models import imagebind_model\n",
        "from imagebind.models.imagebind_model import ModalityType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fx976AgtfK95"
      },
      "outputs": [],
      "source": [
        "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
        "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
        "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ipONXunfK95",
        "outputId": "781aa4e5-4d1d-43fc-c263-cdcd76d2196e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mimagebind_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimagebind_huge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\imagebind_model.py:480\u001b[0m, in \u001b[0;36mimagebind_huge\u001b[1;34m(pretrained)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimagebind_huge\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mImageBindModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_num_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_num_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_drop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimu_drop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.checkpoints/imagebind_huge.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\imagebind_model.py:92\u001b[0m, in \u001b[0;36mImageBindModel.__init__\u001b[1;34m(self, video_frames, kernel_size, audio_kernel_size, audio_stride, out_embed_dim, vision_embed_dim, vision_num_blocks, vision_num_heads, audio_embed_dim, audio_num_blocks, audio_num_heads, audio_num_mel_bins, audio_target_len, audio_drop_path, text_embed_dim, text_num_blocks, text_num_heads, depth_embed_dim, depth_kernel_size, depth_num_blocks, depth_num_heads, depth_drop_path, thermal_embed_dim, thermal_kernel_size, thermal_num_blocks, thermal_num_heads, thermal_drop_path, imu_embed_dim, imu_kernel_size, imu_num_blocks, imu_num_heads, imu_drop_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_preprocessors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_preprocessors(\n\u001b[0;32m     76\u001b[0m     video_frames,\n\u001b[0;32m     77\u001b[0m     vision_embed_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     imu_embed_dim,\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_trunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_modality_trunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_heads(\n\u001b[0;32m    118\u001b[0m     out_embed_dim,\n\u001b[0;32m    119\u001b[0m     vision_embed_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     imu_embed_dim,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_postprocessors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_postprocessors(\n\u001b[0;32m    128\u001b[0m     out_embed_dim\n\u001b[0;32m    129\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\imagebind_model.py:315\u001b[0m, in \u001b[0;36mImageBindModel._create_modality_trunks\u001b[1;34m(self, vision_embed_dim, vision_num_blocks, vision_num_heads, text_embed_dim, text_num_blocks, text_num_heads, audio_embed_dim, audio_num_blocks, audio_num_heads, audio_drop_path, depth_embed_dim, depth_num_blocks, depth_num_heads, depth_drop_path, thermal_embed_dim, thermal_num_blocks, thermal_num_heads, thermal_drop_path, imu_embed_dim, imu_num_blocks, imu_num_heads, imu_drop_path)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SimpleTransformer(\n\u001b[0;32m    294\u001b[0m         embed_dim\u001b[38;5;241m=\u001b[39membed_dim,\n\u001b[0;32m    295\u001b[0m         num_blocks\u001b[38;5;241m=\u001b[39mnum_blocks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m         post_transformer_layer\u001b[38;5;241m=\u001b[39mEinOpsRearrange(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml b d -> b l d\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    312\u001b[0m     )\n\u001b[0;32m    314\u001b[0m modality_trunks \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 315\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mVISION] \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_trunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_transformer_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bias_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mTEXT] \u001b[38;5;241m=\u001b[39m instantiate_trunk(\n\u001b[0;32m    324\u001b[0m     text_embed_dim,\n\u001b[0;32m    325\u001b[0m     text_num_blocks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m     drop_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    330\u001b[0m )\n\u001b[0;32m    331\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mAUDIO] \u001b[38;5;241m=\u001b[39m instantiate_trunk(\n\u001b[0;32m    332\u001b[0m     audio_embed_dim,\n\u001b[0;32m    333\u001b[0m     audio_num_blocks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     drop_path\u001b[38;5;241m=\u001b[39maudio_drop_path,\n\u001b[0;32m    338\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\imagebind_model.py:293\u001b[0m, in \u001b[0;36mImageBindModel._create_modality_trunks.<locals>.instantiate_trunk\u001b[1;34m(embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstantiate_trunk\u001b[39m(\n\u001b[0;32m    291\u001b[0m     embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path\n\u001b[0;32m    292\u001b[0m ):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimpleTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMultiheadAttention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_bias_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bias_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transformer_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_transformer_ln\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdentity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEinOpsRearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb l d -> l b d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_transformer_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEinOpsRearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml b d -> b l d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\transformer.py:212\u001b[0m, in \u001b[0;36mSimpleTransformer.__init__\u001b[1;34m(self, attn_target, embed_dim, num_blocks, block, pre_transformer_layer, post_transformer_layer, drop_path_rate, drop_path_type, norm_layer, mlp_ratio, ffn_dropout_rate, layer_scale_type, layer_scale_init_value, weight_init_style)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown drop_path_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_path_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    213\u001b[0m         block(\n\u001b[0;32m    214\u001b[0m             dim\u001b[38;5;241m=\u001b[39membed_dim,\n\u001b[0;32m    215\u001b[0m             attn_target\u001b[38;5;241m=\u001b[39mattn_target,\n\u001b[0;32m    216\u001b[0m             mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[0;32m    217\u001b[0m             ffn_dropout_rate\u001b[38;5;241m=\u001b[39mffn_dropout_rate,\n\u001b[0;32m    218\u001b[0m             drop_path\u001b[38;5;241m=\u001b[39mdpr[i],\n\u001b[0;32m    219\u001b[0m             norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[0;32m    220\u001b[0m             layer_scale_type\u001b[38;5;241m=\u001b[39mlayer_scale_type,\n\u001b[0;32m    221\u001b[0m             layer_scale_init_value\u001b[38;5;241m=\u001b[39mlayer_scale_init_value,\n\u001b[0;32m    222\u001b[0m         )\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks)\n\u001b[0;32m    224\u001b[0m     ]\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer \u001b[38;5;241m=\u001b[39m post_transformer_layer\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_init_style \u001b[38;5;241m=\u001b[39m weight_init_style\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\transformer.py:213\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown drop_path_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_path_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m--> 213\u001b[0m         \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_scale_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_scale_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks)\n\u001b[0;32m    224\u001b[0m     ]\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer \u001b[38;5;241m=\u001b[39m post_transformer_layer\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_init_style \u001b[38;5;241m=\u001b[39m weight_init_style\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\imagebind\\models\\transformer.py:123\u001b[0m, in \u001b[0;36mBlockWithMasking.__init__\u001b[1;34m(self, dim, attn_target, mlp_ratio, act_layer, norm_layer, ffn_dropout_rate, drop_path, layer_scale_type, layer_scale_init_value)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    121\u001b[0m     attn_target, nn\u001b[38;5;241m.\u001b[39mModule\n\u001b[0;32m    122\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_target should be a Callable. Otherwise attn_target is shared across blocks!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path)\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torch\\nn\\modules\\activation.py:987\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn \u001b[38;5;241m=\u001b[39m add_zero_attn\n\u001b[1;32m--> 987\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torch\\nn\\modules\\activation.py:991\u001b[0m, in \u001b[0;36mMultiheadAttention._reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[1;32m--> 991\u001b[0m         \u001b[43mxavier_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m         xavier_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight)\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torch\\nn\\init.py:327\u001b[0m, in \u001b[0;36mxavier_uniform_\u001b[1;34m(tensor, gain)\u001b[0m\n\u001b[0;32m    324\u001b[0m std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(fan_in \u001b[38;5;241m+\u001b[39m fan_out))\n\u001b[0;32m    325\u001b[0m a \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\giorg\\Projects\\Adorno\\Visually-Indicated-Sounds\\ImageBind\\lib\\site-packages\\torch\\nn\\init.py:14\u001b[0m, in \u001b[0;36m_no_grad_uniform_\u001b[1;34m(tensor, a, b)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_uniform_\u001b[39m(tensor, a, b):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiate model\n",
        "model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3FoAzrwZigvT"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, Normalize\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def transform_and_sample_video_tensor(\n",
        "    video_tensor,\n",
        "    device,\n",
        "    clip_duration=2,\n",
        "    clips_per_video=5,\n",
        "    # Assume video_tensor is in (num_frames, x, y, 3) format\n",
        "):\n",
        "    video_transform = Compose([\n",
        "        Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])]\n",
        "    )\n",
        "\n",
        "    total_frames = video_tensor.shape[0]\n",
        "    frames_per_clip = int(total_frames / clips_per_video)\n",
        "\n",
        "    all_clips = []\n",
        "    for i in range(clips_per_video):\n",
        "      start_frame = i*frames_per_clip\n",
        "      end_frame = start_frame + frames_per_clip\n",
        "      clip = video_tensor[start_frame:end_frame]\n",
        "\n",
        "      clip = clip/255\n",
        "      clip = clip.permute(0, 3, 1, 2)\n",
        "      clip = video_transform(clip)\n",
        "\n",
        "      all_clips.append(clip)\n",
        "\n",
        "    video_output = torch.stack(all_clips, dim=0).to(device)\n",
        "\n",
        "    return video_output\n",
        "\n",
        "def scale_shortest_side_to(video_tensor, target_size=224):\n",
        "    num_clips, num_frames, channels, height, width = video_tensor.shape\n",
        "    # Calculate the scaling factor\n",
        "    scale_factor = target_size / min(height, width)\n",
        "    # Calculate new dimensions\n",
        "    new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
        "    # Resize\n",
        "    new_clips = []\n",
        "    for clip in video_tensor:\n",
        "        scaled_clip = F.interpolate(clip, size=(new_height, new_width), mode='bilinear', align_corners=False)\n",
        "        new_clips.append(scaled_clip)\n",
        "\n",
        "    scaled_video = torch.stack(new_clips, dim=0)\n",
        "    return scaled_video\n",
        "\n",
        "def scale_and_crop_to_target(video_tensor, target_size=224):\n",
        "    num_clips, num_frames, channels, height, width = video_tensor.shape\n",
        "    # Calculate the scaling factor to scale the shortest side to target_size\n",
        "    scale_factor = target_size / min(height, width)\n",
        "    # Calculate new dimensions\n",
        "    new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
        "\n",
        "    # Initialize a list to hold the processed clips\n",
        "    new_clips = []\n",
        "\n",
        "    for clip in video_tensor:\n",
        "        # Scale each clip\n",
        "        # We need to permute the dimensions of the clip to [num_frames, channels, height, width] for F.interpolate\n",
        "        clip = clip.permute(0, 2, 3, 1).float()  # Changing to [num_frames, height, width, channels]\n",
        "        clip = clip.permute(0, 3, 1, 2)  # Now [num_frames, channels, height, width], suitable for F.interpolate\n",
        "        scaled_clip = F.interpolate(clip, size=(new_height, new_width), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # After scaling, we crop the center to ensure it's 224x224\n",
        "        # Calculate the start point for cropping\n",
        "        crop_start_height = max((new_height - target_size) // 2, 0)\n",
        "        crop_start_width = max((new_width - target_size) // 2, 0)\n",
        "\n",
        "        # Crop\n",
        "        cropped_clip = scaled_clip[:, :, crop_start_height:crop_start_height + target_size, crop_start_width:crop_start_width + target_size]\n",
        "\n",
        "        # Permute back to [num_frames, height, width, channels] before appending\n",
        "        cropped_clip = cropped_clip.permute(0, 2, 3, 1)\n",
        "\n",
        "        new_clips.append(cropped_clip)\n",
        "\n",
        "    # Stack all the processed clips together\n",
        "    scaled_and_cropped_video = torch.stack(new_clips, dim=0)\n",
        "\n",
        "    # Ensure the output is of the same dtype as the input (likely uint8 if input are images)\n",
        "    scaled_and_cropped_video = scaled_and_cropped_video.to(dtype=video_tensor.dtype)\n",
        "\n",
        "    return scaled_and_cropped_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbjHCqron9kZ"
      },
      "source": [
        "**Create Dataset Instance and process video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9dDzI9t3hcRG"
      },
      "outputs": [],
      "source": [
        "from VisuallyIndicatedSounds.utils.datasets import StronglyLabelledDataset\n",
        "\n",
        "dataset = StronglyLabelledDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnH30dAxwi8L",
        "outputId": "1bf99f1c-a1aa-444a-d51c-ec16a99fc565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(min(video.size(2), video.size(3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "xIHgXRC7nKF5",
        "outputId": "c5b208fc-460f-4b24-fa67-3332c30fe58f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video Shape: torch.Size([235, 360, 480, 3])\n",
            "Audio Shape: torch.Size([1, 428032])\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"labels_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"segment_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"--HXYSM3ydo_2000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.569,\n        \"min\": 0.0,\n        \"max\": 7.138,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          7.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_time_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.413,\n        \"min\": 4.877,\n        \"max\": 9.703,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9.703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"/m/03m9d0z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "labels_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5952bb1b-5ae2-4c5e-86c4-d9aa712a811c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment_id</th>\n",
              "      <th>start_time_seconds</th>\n",
              "      <th>end_time_seconds</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>--HXYSM3ydo_2000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.877</td>\n",
              "      <td>/m/05zppz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>--HXYSM3ydo_2000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>9.703</td>\n",
              "      <td>/m/03m9d0z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>--HXYSM3ydo_2000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>9.703</td>\n",
              "      <td>/t/dd00066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>--HXYSM3ydo_2000</td>\n",
              "      <td>7.138</td>\n",
              "      <td>9.703</td>\n",
              "      <td>/m/0912c9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5952bb1b-5ae2-4c5e-86c4-d9aa712a811c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5952bb1b-5ae2-4c5e-86c4-d9aa712a811c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5952bb1b-5ae2-4c5e-86c4-d9aa712a811c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a753e7b6-5f7b-47f2-ac8b-90e5c8e4d454\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a753e7b6-5f7b-47f2-ac8b-90e5c8e4d454')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a753e7b6-5f7b-47f2-ac8b-90e5c8e4d454 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f71dc7bc-289a-413d-aaca-2eadeb259d53\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('labels_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f71dc7bc-289a-413d-aaca-2eadeb259d53 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('labels_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         segment_id  start_time_seconds  end_time_seconds       label\n",
              "0  --HXYSM3ydo_2000               0.000             4.877   /m/05zppz\n",
              "1  --HXYSM3ydo_2000               0.000             9.703  /m/03m9d0z\n",
              "2  --HXYSM3ydo_2000               0.000             9.703  /t/dd00066\n",
              "3  --HXYSM3ydo_2000               7.138             9.703   /m/0912c9"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video, audio, labels_df = dataset[0]\n",
        "\n",
        "print(f\"Video Shape: {video.shape}\")\n",
        "print(f\"Audio Shape: {audio.shape}\")\n",
        "\n",
        "labels_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-aDlzgnebr",
        "outputId": "29fed116-f586-400d-bd51-814c34255da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video Shape: torch.Size([5, 47, 3, 360, 480])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 47, 224, 224, 3])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Working with the example video\n",
        "## Required Transformations (Overwriting imagebind.data.load_and_transform_video)\n",
        "scaled_video = transform_and_sample_video_tensor(video, device)\n",
        "\n",
        "print(f\"Video Shape: {scaled_video.shape}\")\n",
        "scaled_video = scale_and_crop_to_target(scaled_video)\n",
        "\n",
        "scaled_video.shape ## (num_clips, frames_per_clip, channels, width, height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy_y2fNWt8Mk",
        "outputId": "cb75341d-3878-45d2-a7af-cd548714c67a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 47, 3, 224, 224])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_video = scaled_video.permute(0, 1, 4, 2, 3)\n",
        "scaled_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV2zV9c7yg-7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZwXJ_ByqmuRh"
      },
      "outputs": [],
      "source": [
        "inputs = {\n",
        "    ModalityType.VISION: scaled_video,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UOYMQKnhoTQ2"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNCsc9xHoUuT",
        "outputId": "fd2c67af-4eed-470c-bc5d-0395247b460a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 1024])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs['vision'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ACINGQsMyq0W"
      },
      "outputs": [],
      "source": [
        "embeddings = outputs['vision']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPqgcEFyy3x8",
        "outputId": "967039b2-7198-43e3-8081-cab0b3a0f0d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0302,  0.0109, -0.0077,  ...,  0.0244, -0.0118, -0.0253],\n",
              "        [ 0.0201,  0.0096, -0.0142,  ...,  0.0157, -0.0169, -0.0196],\n",
              "        [ 0.0185,  0.0028, -0.0041,  ...,  0.0045,  0.0094, -0.0254],\n",
              "        [ 0.0139, -0.0088, -0.0198,  ..., -0.0144,  0.0258, -0.0004],\n",
              "        [ 0.0097, -0.0125, -0.0110,  ..., -0.0187,  0.0265,  0.0056]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M3lQpsyy4VR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
